---
layout: posts
title:  "텍스트 분석"
---
# 텍스트 분석에 관한 기초지식

NLP : 인간의 언어 이해/해석  
텍스트 분석 : 정보 추출 - 비즈니스 인텔리전스/예측 분석 등

텍스트 분석  
- 분류 : 스팸 메일 검출
- 감성 분석 : 긍/부정 분석(지도&비지도)
- 요약 : 중요한 주제/중심 사상 추출
- 군집화/유사도 측정 : 비지도 텍스트 분류

텍스트 분석 머신러닝(데이터 : Feature들의 패턴 학습 후 예측)  
데이터 가공 -> Feature Vectorization(BOW, Word2Vec)

주요 패키지 : NLTK, gensim, spacy  
NLTK : 실제 대량의 데이터 기반에서는 제대로 활용X.  
gensim : 토픽 모델링 분야. Word2Vec 기능 제공.  
spacy : 뛰어난 수행 성능.  

## 텍스트 전처리(정규화)  
- 클렌징 : HTML/XML 태그나 특정 기호 사전에 제거
- 토큰화 : 문장토큰화 or 단어 토큰화 or n-gram
- 필터링/스톱워드 제거/철자 수정 : 불필요한 단어, 분석에 큰 의미가 없는 단어 등(a, the, is, will)
- Stemming/Lemmatization : 어근(단어 원형)추출, Lemmatization이 Stemming보다 정교하고 의미론적 기반에서 단어 원형을 찾아줌


N-gram : 개별 단어로 토큰화 -> 문맥적 의미가 무시되는 것을 해결하고자 도입  
window 단위로 오른쪽으로 sliding

====  

## 텍스트의 피처 벡터화 유형  
- BOW
 개별 단어들을 모두 feature로 만듦  
 문서별로 특정 단어가 나타난 횟수를 count  
 단어에 대해 빈도 값을 부여해 Feature 값을 추출함  
 문맥이나 순서를 고려하지 않는 기법  
 총 문서 M, 총 단어 : N => 전체 데이터 구조 MxN 행렬  
 쉽고 빠른 구축 예상보다 문서의 특징을 잘 나타냄 <=> 문맥 의미 반영x, 희소 행렬 문제

  - 단순 카운트 기반의 벡터화 : 각 문서에서 해당 단어가 나타나는 횟수를 부여  
  - TF-IDF(Term Frequency Inverse Document Frequency) 벡터화 : 자주 나타나는 단어에 높은 가중치 but 자주 나타나는 단어에 대해서는 패널티 부여  
	  TF : 문서에서 해당 단어가 얼마나 나왔는지  
	  DF : 해당 단어가 몇 개의 문서에서 나타났는지  
	  IDF : N/DF   
	  TFIDF_i : TF_i * log(N/DF_i) TF_i : 개별 문서에서의 단어 i 빈도, DF_i : 단어 i를 가지고 있는 문서 개수, N : 전체 문서 개수  
	  특정 문서에서만 나타남 => 해당 문서의 특징을 잘 나타내는 단어일 수도 있음.  
	  모든 문서에서 나타남 => 개별 문서를 특징짓는 정보로서의 의미 상실  
- Word2Vec(Word Embedding)  
 개별 단어를 차원축에 기반해서 단어를 Mapping  
 (King, Queen, Man, Woman : 성별, 지위)  

====

## 사이킷런 CounterVectorizer 파라미터
- max_df : 최대 빈도수 제한(넘어가는 단어를 제외시킴)
- min_df : 최소 빈도수 제한(넘지않는 단어를 제외시킴)
- max_features : 피처로 추출하는 피처의 개수를 제한(정수). 높은 빈도를 가지는 순으로 정렬 후 추출
- stop_words : 'english'로 지정하면 영어의 스톱 워드로 지정된 단어는 추출에서 제외(a, the..)
- ngram_range : (범위 최솟값, 범위 최댓값) 지정. (1,2) : 토큰화된 단어를 해당 범위의 값(window = 1, window = 2)만큼 묶어서 피처로 추출
- analyzer : 피처 추출을 수행한 단위 지정(default : 'word')
- token_pattern : 토큰화를 수행하는 정규 표현식 패턴 지정. 어근 추출시 외부 함수를 사용할 경우 해당 외부 함수를 인자로 사용
- lower_case :  모든 문자를 소문자로 변경할 것인가(default == True)

====

CSR_matrix : 희소행렬(BOW 유형에서 등장함)  
BOW의 Vectorization 모델은 너무 많은 0 값이 메모리 공간을 사용.  
COO 형식 : 좌표(Coordinate)방식. 0이 아닌 데이터만 별도의 배열에 저장하고 그 데이터를 가리키는 행과 열의 위치를 별도의 배열로 저장  
=> (1)0이 아닌 데이터 값 배열 (2) 행 위치 배열 (3) 열 위치 배열  
CSR 형식 : COO 형식이 위치 배열값을 중복적으로 가지는 문제를 해결한 방식.  
=> (1)행 위치 배열의 고유값 시작 인덱스 배열 + (2)총 항목 개수 배열(길이값) = (3) 행 위치 배열의 고유값 시작 인덱스 배열 최종  
-> Scipy : coo_matrix(), csr_matrix() 함수 사용  

====  
20 Newsgroup 분류하기  
과정 : 텍스트 정규화 -> 피처 벡터화 -> ML 학습/예측/평가 -> Pipeline 적용(피처벡터화+ML) -> GridSearchCV 최적화  
====  
scikit learn : fit(), transform(), predict()...  
pipeline = Pipeline([(Name A, Class_A), (Name B, Class_B)])  
pipeline.fit(X_train, y_train) -> 파이프라인 안에 들어있는 객체들을 한번에 처리함  
GridSearchCV :  
params = {  
__parameter(ngram_range, max_df, C..)와 같이 파라미터의 이름을 언더바 2개를 사용해서 표현  
}  
