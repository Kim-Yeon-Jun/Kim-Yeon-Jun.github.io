---
layout: posts
title:  "한국어 자연어처리"
excerpt: "KoNLPy"
date: 2024-07-08 22:12:00 +0900
categories:
  - NLP
tags:
  - KoNLPy
  - Korean Text Data Preprocessing

toc: true
toc_label: "Contents"
toc_sticky: true
toc_headings: "h1, h2, h3"
---
# 자연어 코퍼스 수집

## Corpus??
사전적 의미 : 말뭉치 = 대량의 텍스트 데이터
- 풀고 싶은 NLP 작업의 특성을 잘 담을 수 있는 다양한 패턴의 데이터를 가지고 있어야 함
- 데이터가 유의미한 규모로 확보됨 & 대표성을 가짐

## 자연어 Corpus 수집과정

1. 필요한 자연어 코퍼스 유형 결정
- 문제 정의
- 문제 해결을 위한 솔루션 설정
- 언어 종류, Corpus 종류, Corpus 규모 등등
- 데이터 패턴의 다양성, 데이터 규모, 작업의 복잡도, 언어 모델의 크기
2. 자료 탐색 및 수집
- 공개된 데이터셋
- 벤치마크 데이터셋(KLUE)
- 웹 크롤링(저작권 문제 주의)
- 온라인 뉴스 기사(실시간 데이터 수집 가능)
- 자체 데이터셋 구축 & 라벨링
- 생성형 AI를 이용한 데이터 수집 및 라벨링


## 자연어 전처리 과정
토큰화(Tokenization), 정제(Cleaning), 정규화(Normalization)

# 토큰화(Tokenization)

## 토큰화??
자연어 Corpus를 최소 의미 단위인 토큰 단위로 나누는 작업
단락과 문장을 보다 쉽게 의미를 할당할 수 있는 더 작은 단위로 분할

### 토큰화의 필요성
1. 언어 모델의 자연어 이해 능력 향상
- 작은 단위로 나눌수록 더 세밀한 수준으로 자연어를 이해할 수 있음
- 통계적으로 모델이 특정 단어의 빈도와 자주 나타나는 위치를 게산할 수 있게 해줌

2. 다양한 자연어를 효율적으로 표현 가능
- 텍스트가 바뀌어서 입력으로 들어온 경우, 문장 단위로 이해한 LM보다 단어 단위로 이해한 LM이 단어 간 관계 파악에 용이
- 중복을 제거한 대규모 자연어 Corpus 내 총 토큰의 집합(set)을 단어 사전(Vocabulary)이라 부름

## 토큰화 방법(토큰의 기준에 따라 분류)
- 문장(Sentence) 토큰화 : 문장 부호를 사용하여 구분할 수 있으나 메일형식, 특수기호로 사용된 경우 예외가 발생함
- 단어(Word) 토큰화 : 보편적인 방식으로 ' '(띄어쓰기or공백)을 구분자로 사용. 한국어는 교착어로 단어 토큰화에 대한 성능이 좋지 않음
  - 교착어 : 어근과 접사, 조사에 의해 단어의 기능이 결정되는 언어
  - 문제점
    1. OOV(Out Of Vocabulary)
      - 학습하지 않은 단어가 모델 예측시 입력으로 들어오는 경우 발생
      - UNK(Unknown Token)을 사용하여 임시적으로 해결
      - OOV에 해당하는 단어가 많은 경우 LM 성능에 안좋은 영향을 줄 수 있음

    2. 새로운 단어의 추가 = 사전의 크기 증가
      - OOV를 해결하기 위해 계속해서 사전의 규모를 키움
      - 모델이 사전에 포함된 단어를 찾는데 소요되는 시간도 같이 증가함
- 문자(Character) 토큰화 : Corpus를 문자 단위로 분리하는 것
    - 안녕 => ㅇ, ㅏ, ㄴ, ㄴ, ㅕ, ㅇ
    - 문제점
      1. 문장 1개 생성하는데에 많은 추론이 필요함(1개의 단어 -> 6 추론)
      2. 단어 사전은 작지만 모델의 예측 시간에 문제가 생길 수 있음(토큰간 관계 학습의 난이도 증가)
- 서브워드(Sub-Word) 토큰화 : 단어, 문자 토큰화를 해결하기 위한 방법
    - 토큰의 단위를 n개의 문자(n-gram)로 정의하고 해당 기준에 따라 텍스트 분절
    - 형태소 분절 기반의 서브워드 토큰화로 확장될 수 있어서 한국어에도 좋은 성능을 보임
    - ex. BPE(Byte Pair Encoding)

# 서브워드 토큰화(BPE :Byte Pair Encoding)

## 서브워드??
일반적으로 정의하는 단어보다 더 작은 의미의 단위를 뜻함
ex) 아침밥 => 아침 + 밥

## 서브워드 토큰화 이점
토큰의 단위를 서브워드로 설정 -> 희귀단어, 신조어에 대한 대응이 가능해짐
영어나 한국어의 경우 의미있는 단위로 나누는 것이 어느정도 가능함

## BPE 알고리즘(Byte Pair Encoding)
Corpus 내 단어의 등장 빈도에 따라 서브워드를 구축하는 알고리즘
글자(Character) 단위로 분리한 후 등장빈도에 따라 글자들을 서브워드 집합으로 만들어내는 bottom-up 방식
가장 빈도수가 높은 서브워드 쌍을 하나의 서브워드로 통합하는 과정
ex)
{"low":5 ,"lower":2, "newest":6, "widest":3} =>(글자 단위로 분리)=> 초기 단어 사전 : {l, o, w, e, r, n, s, t, i, d}
~...~ Vocabluary : {l, o, w, e, r, n, s, t, i, d, es, est, lo, low, ne, new, newest, wi, wid, widest}
lowest라는 새로운 단어가 입력되면 이를 단어사전의 low와 est를 통해 표현할 수 있게됨.
```python
import re, collections

# 단어 쌍의 빈도 계산
def get_freq(dictionary):
  pairs = collections.defaultdict(int)

  for word, freq in dictionary.items():
    symbols = word.split()
    for i in range(len(symbols)-1):
      pairs[symbols[i], symbols[i+1]] += freq
  print("현재 pair들의 빈도수 : ", dict(pairs))
  return pairs

# 단어 쌍을 단일 토큰으로 병합
def merge_dictionary(target_pair, input_dict):
  output_dict = []
  bigram = re.escape(' '.join(target_pair))
  p = re.compiler(r'(?<!\S)' + bigram + r'(?!\S)')
  for word in input_dict:
    w_out = p.sub(' '.join(target_pair), word)
    output_dict[w_out] = input_dict[word]
  return output_dict
```

## WordPiece Tokenizer(BPE의 변형 알고리즘)
ex)
unable -> un, ##able 이라고 할 때, 출현빈도가 높다면 병합X, 출현빈도가 낮다면 병합

생성 모델의 경우 BPE, 자연어 이해 모델(BERT)의 경우 WordPiece Tokenizer를 주로 사용함

# 자연어 데이터 전처리(정제, 정규화)

##정제(Cleaning) & 정규화(Normalization)
토큰화 작업 전,후에 텍스트 데이터를 용도에 맞게 정제, 정규화하는 작업이 필요함
정제 : 보유하고 있는 Corpus에서 노이즈 데이터를 제거하는 것
정규화 : 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만드는 것

## 정제작업
토큰화에 방해가 되는 부분들을 필터링 혹은 토큰화 작업 이후에도 남아있는 노이즈 제거를 위한 지속적으로 이루어지는 전처리 과정
(어떤 특성이 노이즈인지, 어느정도까지 제거할 것인지에 대한 판단이 필요함)
토큰화 전 : 토큰화 작업에 방해되는 특성들에 대해 필터링
토큰화 후 : 자연어처리 작업을 이해하는데 방해가 되는 특성(노이즈)에 대해 필터링
- 불용어(Stopword)처리
불용어 : 문장 내에서 빈번하게 발생하여 의미를 부여하기 어려운 단어
ex) a, the와 같은 모든 구문에 등장 & 단어 자체에 의미(정보)가 거의 없음
정보량이 적은 단어들을 토큰에서 제거하여 단어 사전을 효율적으로 관리할 수 있고 이에 따라 모델의 응답시간을 단축시킬 수 있다.
```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')

from nltk.corpus import stopwords
english_stopwords = stopwords.words('english')
print(*english_stopwords, sep='\n')

#사전 정의한 불용어만 제거
from nltk.tokenize import word_tokenize
eng_sentence = "...."
stop_words = set(stopwords.words('english'))
tokens = word_tokenize(eng_sentence)
cleaned_tokens = []
for t in tokens:
  if t not in stop_words:
    cleaned_tokens.append(t)

#stop_words를 임의로 선언하여 사용할 수도 있음
```

각자 상황에 맞게 불용어 사전을 이용 혹은 추가하여 작업을 진행해야하므로 사전에 불용어에 대해 설정을 하고 진행하는 것이 좋다.

- 불필요한 태그 및 특수 문자 제거
웹 크롤링의 결과로 태그, 특수 문자들이 Corpus에 포함되어 있는 경우 토큰화 전 단계에서 정제
파이썬의 정규표현식을 이용하여 이를 제거할 수 있다.

```python
#HTML 태그 제거
import re

pattern = r"<.*?>"
text = "<p>This is a paragraph.</p><a href='http://example.com'>Link</a>"
clean_text = re.sub(pattern, "", text)
print("HTML 태그 제거된 텍스트:", clean_text)

#이메일 주소 패턴
import re

pattern = r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"
text = "user@example.com"
if re.match(pattern, text):
    print("유효한 이메일 주소입니다.")
else:
    print("유효하지 않은 이메일 주소입니다.")
```

- Corpus 내 등장 빈도가 적은 단어 제거
빈도 수가 너무 적어 자연어 처리에 도움이 되지 않는 단어(정보량 적은 단어) 제거
Corpus 내 단어들의 빈도를 분석하여 분포를 보고 특정 임계값(threshold)를 기준으로 임계값 아래의 단어들을 필터링
@와 같은 특수 문자의 경우 일반적인 작업에서는 정보량이 적은 토큰일 수도 있으나 
이메일 관련 데이터에서는 유용한 토큰으로 사용할 수 있기에 발생할 수 있는 문제를 확인해보고 사용해야한다.

## 정규화
정규화 : 일반적인 머신러닝 작업의 경우, 학습 데이터 값들이 적당한 범위를 유지하도록 데이터의 범위를 변환/스케일링 하는 과정
feature의 값이 큰 차이를 보이는 경우 모델에서 데이터의 해석에서 이상한 결과를 도출할 수도 있음
따라서 모든 데이터가 같은 정도의 Scale(=중요도)로 반영되도록 해야한다.
자연어처리에서의 정규화의 핵심은 표현 방법이 다르지만 의미가 동일한 단어들을 통합하여 같은 단어로 만들어주는 것
단어들을 통합함으로서 통합된 단어를 기준으로 빈도수를 계산할 수 있게됨
- 어간 추출(Stemming) & 표제어 추출(Lemmatization)
서로 다른 단어를 하나의 단어로 일반화 시켜 문서 내에 추출되는 단어의 개수를 줄임
  - 어간 추출 : 형태학적 분석을 단순화한 것. 정해진 규칙만 보고 단어의 어미를 자르는 어림잡아하는 작업  
     결과가 단어 사전에 존재하지 않을 수도 있음
  - 표제어 추출 : 다른 형태의 단어이더라도 뿌리 단어를 찾는 것으로 단어의 개수를 줄임  ex) am, are, is = be
```python
nltk.download('wordnet')
nltk.download('omw-1 .4')
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

print(lemmatizer.lemmatize('dies', 'v')) # 단어와 단어의 품사를 input으로 제공
```

- 대소문자 통합
영어권에서 사용할 수 있는 정규화 방법으로 대부분의 글이 소문자로 작성됨에 따라 이를 소문자로 변환하는 작업을 주로 함
하지만 US와 us처럼 회사이름, 사람이름 등 다른 뜻으로 구분되어야 하는 경우가 있기 때문에 이러한 예외사항을 고려해야됨

참고)정규화를 진행하는 과정에서 모든 것에 규칙을 두고 진행하는 것은 학습에 악영향을 줄 수 있다.

 
# 한국어 형태소 분석
